{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harris_Lab\\miniconda3\\envs\\splice\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-26 08:17:06,096\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-07-26 08:17:06,706\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ExponentialLR, LinearLR\n",
    "\n",
    "from splice import splice_model\n",
    "from splice.base import *\n",
    "from splice.loadCellsFile import *\n",
    "from splice.utilities import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\Harris_Lab\\Projects\\SPLICE\\data\\uberphys\\split_rates.npz\"\n",
    "split_rates = np.load(filepath)\n",
    "train_ads = torch.Tensor(split_rates['train_ads']).to(device)\n",
    "train_m1 = torch.Tensor(split_rates['train_m1']).to(device)\n",
    "test_ads = torch.Tensor(split_rates['test_ads']).to(device)\n",
    "test_m1 = torch.Tensor(split_rates['test_m1']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = splice_model(\n",
    "    n_a = train_ads.shape[1],\n",
    "    n_b = train_m1.shape[1],\n",
    "    n_shared = 2,\n",
    "    n_priv_a = 2,\n",
    "    n_priv_b = 2,\n",
    "    layers_enc = [20,20],\n",
    "    layers_dec = [20],\n",
    "    layers_msr = [20,20],\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t source loss: 41.0616 | 40.3284 \t target loss: 257.0844 | 264.1692 \t disent loss: 1.0000 | 0.0823 \t msr loss: 0.0000 | 4.8834\n",
      "Epoch 500 \t source loss: 18.0103 | 18.1820 \t target loss: 67.5321 | 79.5731 \t disent loss: 1.0000 | 0.4374 \t msr loss: 0.0000 | 13.1859\n",
      "Epoch 1000 \t source loss: 16.6460 | 17.2174 \t target loss: 58.6734 | 76.2645 \t disent loss: 1.0000 | 1.7294 \t msr loss: 0.0000 | 12.8936\n",
      "Epoch 1500 \t source loss: 16.0966 | 16.8654 \t target loss: 47.3271 | 63.0033 \t disent loss: 1.0000 | 4.7268 \t msr loss: 0.0000 | 19.6811\n",
      "Epoch 2000 \t source loss: 15.8241 | 16.7150 \t target loss: 45.3304 | 55.4812 \t disent loss: 1.0000 | 7.1361 \t msr loss: 0.0000 | 27.9648\n",
      "Epoch 2500 \t source loss: 15.6301 | 16.6984 \t target loss: 43.5824 | 51.3354 \t disent loss: 1.0000 | 10.1719 \t msr loss: 0.0000 | 40.0016\n",
      "Epoch 3000 \t source loss: 15.4599 | 16.7443 \t target loss: 42.3436 | 49.0680 \t disent loss: 1.0000 | 14.1910 \t msr loss: 0.0000 | 53.7456\n",
      "Epoch 3500 \t source loss: 15.3343 | 16.7387 \t target loss: 41.3548 | 47.6054 \t disent loss: 1.0000 | 18.6916 \t msr loss: 0.0000 | 70.0226\n",
      "Epoch 4000 \t source loss: 15.2416 | 16.6944 \t target loss: 40.5782 | 46.7094 \t disent loss: 1.0000 | 23.4393 \t msr loss: 0.0000 | 89.2370\n",
      "Epoch 4500 \t source loss: 15.1693 | 16.6601 \t target loss: 39.4142 | 45.7299 \t disent loss: 1.0000 | 27.5960 \t msr loss: 0.0000 | 103.9285\n",
      "Epoch 5000 \t source loss: 15.1048 | 16.6174 \t target loss: 39.0780 | 45.5343 \t disent loss: 0.0644 | 0.1318 \t msr loss: 1.9010 | 1.9135\n",
      "Epoch 5500 \t source loss: 15.0817 | 16.5769 \t target loss: 38.7577 | 45.1942 \t disent loss: 0.0799 | 0.1622 \t msr loss: 1.8499 | 1.8642\n",
      "Epoch 6000 \t source loss: 15.0537 | 16.5094 \t target loss: 38.4823 | 45.1840 \t disent loss: 0.0634 | 0.1309 \t msr loss: 2.0046 | 2.0123\n",
      "Epoch 6500 \t source loss: 15.0252 | 16.5524 \t target loss: 38.2957 | 45.2849 \t disent loss: 0.0608 | 0.1217 \t msr loss: 1.9001 | 1.9062\n",
      "Epoch 7000 \t source loss: 14.9980 | 16.5439 \t target loss: 38.0971 | 45.2148 \t disent loss: 0.0641 | 0.1323 \t msr loss: 1.9555 | 1.9576\n",
      "Epoch 7500 \t source loss: 14.9801 | 16.5104 \t target loss: 37.9710 | 45.3343 \t disent loss: 0.0736 | 0.1476 \t msr loss: 1.8668 | 1.8720\n",
      "Epoch 8000 \t source loss: 14.9579 | 16.5258 \t target loss: 37.8875 | 45.3750 \t disent loss: 0.0559 | 0.1129 \t msr loss: 1.9301 | 1.9326\n",
      "Epoch 8500 \t source loss: 14.9419 | 16.5265 \t target loss: 37.7925 | 45.4423 \t disent loss: 0.0728 | 0.1480 \t msr loss: 1.8633 | 1.8749\n",
      "Epoch 9000 \t source loss: 14.9295 | 16.5482 \t target loss: 37.6477 | 45.2531 \t disent loss: 0.0737 | 0.1492 \t msr loss: 1.9123 | 1.9186\n",
      "Epoch 9500 \t source loss: 14.9166 | 16.5197 \t target loss: 37.4759 | 45.1873 \t disent loss: 0.0825 | 0.1681 \t msr loss: 1.8449 | 1.8578\n",
      "Epoch 10000 \t source loss: 14.9059 | 16.4967 \t target loss: 37.3745 | 45.3918 \t disent loss: 0.0621 | 0.1262 \t msr loss: 1.9515 | 1.9545\n",
      "Epoch 10500 \t source loss: 14.8946 | 16.4920 \t target loss: 37.1872 | 45.1069 \t disent loss: 0.0754 | 0.1534 \t msr loss: 1.8692 | 1.8816\n",
      "Epoch 11000 \t source loss: 14.8907 | 16.4441 \t target loss: 37.0848 | 45.1726 \t disent loss: 0.0612 | 0.1298 \t msr loss: 1.9957 | 1.9977\n",
      "Epoch 11500 \t source loss: 14.8798 | 16.5010 \t target loss: 36.9853 | 45.2417 \t disent loss: 0.0691 | 0.1421 \t msr loss: 1.9042 | 1.9116\n",
      "Epoch 12000 \t source loss: 14.8674 | 16.4495 \t target loss: 36.8753 | 45.2416 \t disent loss: 0.0483 | 0.1010 \t msr loss: 1.9445 | 1.9531\n",
      "Epoch 12500 \t source loss: 14.8631 | 16.4152 \t target loss: 36.7683 | 45.3486 \t disent loss: 0.0687 | 0.1403 \t msr loss: 1.8780 | 1.8885\n",
      "13000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_ads, train_m1, test_ads, test_m1, disent_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, c_disent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\harris_lab\\projects\\splice\\splice\\splice.py:286\u001b[0m, in \u001b[0;36msplice_model.fit\u001b[1;34m(self, a_train, b_train, a_test, b_test, epochs, lr, end_factor, verbose, disent_start, msr_restart, msr_iter_normal, msr_iter_restart, c_disent, device, weight_decay)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreeze_all_except(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_a2b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_b2a)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(msr_iter):\n\u001b[1;32m--> 286\u001b[0m     _, _, _, _, m_b2a, m_a2b, a_hat, b_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(a_train, b_train)\n\u001b[0;32m    288\u001b[0m     msr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsr_scheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Harris_Lab\\miniconda3\\envs\\splice\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\harris_lab\\projects\\splice\\splice\\splice.py:129\u001b[0m, in \u001b[0;36msplice_model.forward\u001b[1;34m(self, a, b)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# measurement networks predict opposite datasets/shared latents\u001b[39;00m\n\u001b[0;32m    128\u001b[0m m_a2b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_a2b(z_a) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_a2b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(b)\n\u001b[1;32m--> 129\u001b[0m m_b2a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_b2a(z_b) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_b2a \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(a)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_a, z_b2a, z_a2b, z_b, m_b2a, m_a2b, a_hat, b_hat\n",
      "File \u001b[1;32mc:\\Users\\Harris_Lab\\miniconda3\\envs\\splice\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\harris_lab\\projects\\splice\\splice\\base.py:142\u001b[0m, in \u001b[0;36mdecoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 142\u001b[0m         x \u001b[38;5;241m=\u001b[39m carlosPlus(layer(x))\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\users\\harris_lab\\projects\\splice\\splice\\base.py:28\u001b[0m, in \u001b[0;36mcarlosPlus\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcarlosPlus\u001b[39m(x):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    Variant of the softplus activation function.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m        Tensor: Output tensor.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (F\u001b[38;5;241m.\u001b[39msoftplus(x) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_ads, train_m1, test_ads, test_m1, disent_start=5000, c_disent=0.5, epochs=50000, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
